# SummerParty Homework 4: Deep Learning Experiments

Этот проект содержит решения для заданий по глубокому обучению, включая сравнение полносвязных и сверточных нейронных сетей, анализ архитектур CNN и эксперименты с кастомными слоями.

## Структура проекта

```
homework/
├── homework_cnn_vs_fc_comparison.py        # Задание 1: Сравнение FC и CNN
├── homework_cnn_architecture_analysis.py    # Задание 2: Анализ архитектур CNN
├── homework_custom_layers_experiments.py    # Задание 3: Кастомные слои и Residual блоки
├── models/
│   ├── fc_models.py                        # Полносвязные модели
│   ├── cnn_models.py                       # Сверточные модели
│   ├── custom_layers.py                    # Кастомные слои
├── utils/
│   ├── comparison_utils.py                 # Утилиты для сравнения
│   ├── visualization_utils.py              # Утилиты для визуализации
│   ├── training_utils.py                   # Утилиты для обучения
├── results/
│   ├── mnist_comparison/                   # Результаты для MNIST
│   ├── cifar_comparison/                   # Результаты для CIFAR-10
│   ├── architecture_analysis/               # Результаты анализа архитектур
│   ├── custom_layers_experiments/           # Результаты кастомных слоев
├── plots/                                  # Все графики по заданиям
├── data/                                   # Данные датасетов
└── README.md                               # Этот файл
```

## Результаты задания 1: Сравнение полносвязной сети и CNN

Результаты производительности моделей на датасете (предположительно CIFAR-10):

| Модель             | Точность на тесте (%) | Время обучения (с) | Время инференса (с) | Количество параметров |
|--------------------|-----------------------|--------------------|---------------------|----------------------|
| Полносвязная сеть  | 97.84                 | 172.02             | 1.8724              | 567,434              |
| Простая CNN        | 99.14                 | 196.49             | 2.1246              | 1,625,866            |
| CNN с Residual     | 98.98                 | 427.19             | 3.2769              | 1,719,242            |
    
### Ключевые наблюдения:
- **Точность**: CNN превосходят FC-сеть на 1.3% (99.14% vs 97.84%)
- **Ресурсы**: Residual-версия увеличивает время обучения на 217% по сравнению с FC, Простая CNN требует в 2.9× больше параметров
- **Компромисс**: Residual-связи незначительно снижают точность (-0.16%), но улучшают устойчивость обучения


## Результаты задания 2: Анализ архитектур CNN

Результаты производительности моделей на датасете CIFAR-10 для различных архитектур CNN:

### Влияние глубины сети

| Модель                  | Точность на тесте (%) | Время обучения (с) | Количество параметров |
|-------------------------|-----------------------|--------------------|----------------------|
| 2 слоя без Residual     | 68.55                 | 216.91             | 1,060,266            |
| 4 слоя без Residual     | 77.83                 | 259.81             | 591,658              |
| 2 слоя с Residual       | 68.51                 | 208.39             | 1,060,266            |
| 4 слоя с Residual       | 77.51                 | 258.46             | 630,954              |

#### Вывод 
- Увеличение глубины с 2 до 4 слоёв даёт +9.28% точности при сокращении параметров на 44%
-  Residual-связи в данной конфигурации:
    Не влияют на точность (±0.32%)
    Снижают время обучения на 0.5%
    Увеличивают параметры на 6.6%

### Влияние размера ядра свертки

| Модель         | Точность на тесте (%) | Время обучения (с) | Количество параметров |
|----------------|-----------------------|--------------------|----------------------|
| Ядра 3x3       | 68.97                 | 246.51             | 2,118,154            |
| Ядра 5x5       | 68.53                 | 265.91             | 2,152,458            |
| Ядра 7x7       | 69.05                 | 321.16             | 2,203,914            |

#### Анализ
-Оптимальный вариант: 3×3 (максимальное соотношение точности и скорости)
- Увеличение ядра до 7×7:
    Даёт +0.08% точности
    Увеличивает время обучения на 30.3%
    Добавляет 85,760 параметров

## Результаты задания 3: Кастомные слои и Residual блоки

Результаты производительности моделей на датасете CIFAR-10 для кастомных слоев и различных Residual блоков:

### Кастомные слои

| Модель                  | Точность на тесте (%) | Время обучения (с) | Количество параметров | Стабильность (дисперсия потерь) |
|-------------------------|-----------------------|--------------------|----------------------|---------------------------------|
| Базовая CNN             | 64.69                 | 195.06             | 2,100,490            | 0.1632                          |
| Кастомная свертка       | 63.37                 | 187.60             | 2,100,491            | 0.0609                          |
| Attention               | 64.86                 | 229.12             | 2,105,691            | 0.1486                          |
| Кастомная активация     | 64.88                 | 186.23             | 2,100,491            | 0.1082                          |
| Кастомный пуллинг       | 63.92                 | 188.81             | 2,100,490            | 0.1786                          |

#### Анализ
- **Базовая CNN**: Базовая точность (64.69%), умеренное время обучения, высокая дисперсия потерь (0.1632).
- **Кастомная свертка**: Чуть ниже точность (63.37%), но лучшая стабильность (дисперсия 0.0609) и быстрее обучение.
- **Attention**: Сравнимая с базовой точность (64.86%), но больше параметров и времени обучения из-за механизма внимания.
- **Кастомная активация**: Сравнимая точность (64.88%), быстрее обучение и умеренная стабильность.
- **Кастомный пуллинг**: Точность ниже базовой (63.92%), но стабильность хуже (дисперсия 0.1786).

### Residual блоки

| Модель                  | Точность на тесте (%) | Время обучения (с) | Количество параметров | Стабильность (дисперсия потерь) |
|-------------------------|-----------------------|--------------------|----------------------|---------------------------------|
| Базовый Residual        | 72.83                 | 503.78             | 2,248,714            | 0.0095                          |
| Bottleneck Residual     | 69.47                 | 353.46             | 2,109,770            | 0.0080                          |
| Wide Residual           | 74.38                 | 660.35             | 2,396,554            | 0.0140                          |

#### Анализ
- **Базовый Residual**: Высокая точность (72.83%) и хорошая стабильность (дисперсия 0.0095), но долгое обучение.
- **Bottleneck Residual**: Ниже точность (69.47%), но меньше параметров и быстрее обучение, лучшая стабильность (дисперсия 0.0080).
- **Wide Residual**: Наивысшая точность (74.38%), но больше всего параметров и времени обучения, умеренная стабильность.


### Примечание
Дополнительные эксперименты с кастомными слоями (с меньшим количеством параметров, около 1,050,954) показали схожие тенденции, но с чуть меньшей точностью (62.91–65.37%). Результаты доступны в `results/custom_layers_experiments/`.

## Вывод
- **Задание 1**: CNN демонстрирует преимущество в точности (99.14% против 97.84% у FC), но требует больше ресурсов — время обучения увеличивается на 14%, а количество параметров в 2.9 раза. Residual CNN сохраняет точность на уровне 98.98%, но время обучения возрастает в 2.5 раза.
- **Задание 2**: Увеличение глубины сети до 4 слоёв даёт прирост точности на 9.28% (77.83% против 68.55%), при этом Residual-связи незначительно влияют на результат. Ядра 7×7 обеспечивают максимальную точность (69.05%), но требуют на 30% больше времени обучения по сравнению с 3×3.
- **Задание 3**: Attention-слой повышает точность на 0.17% с минимальным ростом параметров, а Wide Residual показывает наилучший результат (74.38%), но требует значительных вычислительных затрат. Кастомная свёртка улучшает стабильность обучения, однако снижает точность на 1.32%.